
<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</title>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:30px">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation</span>
	</center>

	<br>


  	<table align=center width=900px>
  	 <tr>
		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="http://wywu.github.io">Wayne Wu</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://ai.stanford.edu/~kaidicao/">Kaidi Cao</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en&oi=sra">Cheng Li</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a></span>
		</center>
		</td>

	 </tr>
	</table>

	<table align=center width=900px>
  	 <tr>
		<td align=center width=80px>
		<center>
		<span style="font-size:20px">SenseTime Research</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:20px">Stanford University</span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:20px">Nanyang Technological University</span>
		</center>
		</td>

	 </tr>
	</table>


<!-- 
	<div class="authors">
	  <a href="http://wywu.github.io">Wayne Wu</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://ai.stanford.edu/~kaidicao/">Kaidi Cao</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en&oi=sra">Cheng Li</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a><sup>3</sup>&nbsp;&nbsp;
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="https://www.sensetime.com/?lang=en-us">SenseTime Research<br></a>
	  <sup>2</sup><a href="http://svl.stanford.edu/">Stanford University<br></a>
	  <sup>3</sup><a href="http://scse.ntu.edu.sg/Pages/Home.aspx">Nanyang Technological University<br></a>
	</div> -->



     
  		  <br>
  		  <table align=center width=900px>
  			  <tr>
  	              <td width=900px>
  					<center>
  	                	<a href="./support/TGaGa.png"><img src = "./support/TGaGa.png" height="800px"></img></href></a><br>
					</center>
  	              </td>
  	          </tr>
  		  </table>

      	  <br>
      	  <p style="text-align:justify">
				Unsupervised image-to-image translation aims at learning a mapping between two visual domains. However, learning a translation across large geometry variations always ends up with failure. In this work, we present a novel disentangle-and-translate framework to tackle the complex objects image-to-image translation task. Instead of learning the mapping on the image space directly, we disentangle image space into a Cartesian product of the appearance and the geometry latent spaces. Specifically, we first introduce a geometry prior loss and a conditional VAE loss to encourage the network to learn independent but complementary representations. The translation is then built on appearance and geometry space separately. Extensive experiments demonstrate the superior performance of our method to other state-of-the-art approaches, especially in the challenging near-rigid and non-rigid objects translation tasks. In addition, by taking different exemplars as the appearance references, our method also supports multimodal translation.
      	  </p>
  		  <br><br>
		  <hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1100>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
				  <td><a href="https://arxiv.org/pdf/1904.09571.pdf"><img style="height:180px" src="./support/paper.png"/></a></td>
				  <td><span style="font-size:14pt">TransGaGa: Geometry-Aware Unsupervised Image-to-Image Translation<br><br>
                          <i>Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, Chen Change Loy</i><br><br>
				  To Appear in Computer Vision and Pattern Recognition, CVPR 2019. <br>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=400px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="https://arxiv.org/pdf/1808.07371.pdf">[PDF]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/TGaGa_bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>
		  	<br>

  		  	<hr>

  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  We would like to thank Kwan-Yee Lin and Jingtan Piao for insightful discussion and their exceptional support.</a>.
			</left>
		</td>
		</tr>
		</table>
		<br><br>
</body>
</html>
