
<!DOCTYPE html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}

	h1 {
		font-weight:300;
	}

	.disclaimerbox {
		background-color: #eee;
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}

	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}

	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}

	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		        15px 15px 0 0px #fff, /* The fourth layer */
		        15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		        20px 20px 0 0px #fff, /* The fifth layer */
		        20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		        25px 25px 0 0px #fff, /* The fifth layer */
		        25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	.vert-cent {
		position: relative;
	    top: 50%;
	    transform: translateY(-50%);
	}

	hr
	{
		border: 0;
		height: 1.5px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
  <head>
		<title>MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</title>
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:30px">MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation</span>
	</center>

	<br>


  	<table align=center width=900px>
  	 <tr>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a>Kaisiyuan Wang*</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a>Linsen Song*</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://wuqianyi.top/">Qianyi Wu*</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://yzhq97.github.io/">Zhuoqian Yang</a></span>
		</center>
		</td>

	 </tr>
	</table>


	<br>


  	<table align=center width=900px>
  	 <tr>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="http://wywu.github.io">Wayne Wu</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=ayrg9AUAAAAJ&hl=en">Ran He</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="https://scholar.google.com/citations?user=gFtI-8QAAAAJ&hl=en">Yu Qiao</a></span>
		</center>
		</td>

		<td align=center width=80px>
		<center>
		<span style="font-size:18px"><a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a></span>
		</center>
		</td>

	 </tr>
	</table>


	<br>
	
	<table align=center width=900px>
  	 <tr>
		<td align=center width=115px>
		<center>
		<span style="font-size:20px">SenseTime Research</span>
		</center>
		</td>

		<td align=center width=115px>
		<center>
		<span style="font-size:20px">Robotics Institute, Carnegie Mellon University</span>
		</center>
		</td>

	 </tr>
	</table>

	<br>

	<table align=center width=900px>
  	 <tr>

		<td align=center width=115px>
		<center>
		<span style="font-size:20px">University of Chinese Academy of Sciences</span>
		</center>
		</td>

		<td align=center width=115px>
		<center>
		<span style="font-size:20px">Nanyang Technological University</span>
		</center>
		</td>

	 </tr>
	</table>

	<br>

<!-- 
	<div class="authors">
	  <a href="http://wywu.github.io">Wayne Wu</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://ai.stanford.edu/~kaidicao/">Kaidi Cao</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=F5rVlz0AAAAJ&hl=en&oi=sra">Cheng Li</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=AerkT0YAAAAJ&hl=en">Chen Qian</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a><sup>3</sup>&nbsp;&nbsp;
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="https://www.sensetime.com/?lang=en-us">SenseTime Research<br></a>
	  <sup>2</sup><a href="http://svl.stanford.edu/">Stanford University<br></a>
	  <sup>3</sup><a href="http://scse.ntu.edu.sg/Pages/Home.aspx">Nanyang Technological University<br></a>
	</div> -->



     
  		  <br>
  		  <table align=center width=900px>
  			  <tr>
  	              <td width=900px>
  					<center>
  	                	<a href="./support/MEAD.png"><img src = "./support/MEAD.png" height="500px"></img></href></a><br>
					</center>
  	              </td>
  	          </tr>
  		  </table>

      	  <br>
      	  <p style="text-align:justify">
				The synthesis of natural emotional reactions is an essential criterion in vivid talking-face video generation. This criterion is nevertheless seldom taken into consideration in previous works due to the absence of a large-scale, high-quality emotional audio-visual dataset. To address this issue, we build the Multi-view Emotional Audio-visual Dataset (MEAD), a talking-face video corpus featuring 60 actors and actresses talking with eight different emotions at three different intensity levels. High-quality audio-visual clips are captured at seven different view angles in a strictly-controlled environment. Together with the dataset, we release an emotional talking-face generation baseline that enables the manipulation of both emotion and its intensity. Our dataset could benefit a number of different research fields including conditional generation, cross-modal understanding and expression recognition.
      	  </p>
  		  <br><br>
		  <hr>
		 <!-- <table align=center width=550px> -->
  		  <table align=center width=1100>
	 		<center><h1>Paper</h1></center>
  			  <tr>
  	              <!--<td width=300px align=left>-->
  	              <!-- <a href="http://arxiv.org/pdf/1603.08511.pdf"> -->
				  <!-- <td><a href="#"><img class="layered-paper-big" style="height:175px" src="./resources/images/paper.png"/></a></td> -->
				  <td><a href="https://arxiv.org/pdf/1904.09571.pdf"><img style="height:180px" src="./support/paper.png"/></a></td>
				  <td><span style="font-size:14pt">MEAD: A Large-scale Audio-visual Dataset for Emotional Talking-face Generation<br><br>
                          <i>Kaisiyuan Wang*, Qianyi Wu*, Linsen Song*, Zhuoqian Yang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, Chen Change Loy</i><br><br>
				  European Conference on Computer Vision, ECCV 2020. <br>
				  </td>
  	              </td>
              </tr>
  		  </table>
		  <br>

		  <table align=center width=400px>
			  <tr>
				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/MEAD.pdf">[PDF]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/MEAD-supp.pdf">[Appendix]</a>
  	              </center></td>

				  <td><span style="font-size:14pt"><center>
				  	<a href="./support/MEAD_bibtex.txt">[Bibtex]</a>
  	              </center></td>
              </tr>
  		  </table>
		  	<br>

  		  	<hr>


  		  <table align=center width=900px>
  		  <center><h1>Dataset (coming soon)</h1></center>
  			  <tr>
  	              <td width=900px>
  					<center>
  	                	<a href="./support/MEAD-data.png"><img src = "./support/MEAD-data.png" height="650px"></img></href></a><br>
					</center>
			<br>
			<p class="text-justify">
				We build the Multi-view Emotional Audio-visual Dataset (MEAD), a talking-face video corpus featuring 60 actors talking with eight different emotions at three different intensity levels (except for neutral). The videos are simultaneously recorded at seven different perspectives in a strictly-controlled environment to provide high-quality details of facial expressions. About 40 hours of audio-visual clips are recorded for each person and view.
			</p>
		   	<div>
				<center><a href="" class="btn btn-outline-secondary">[Download]</a></center>
		   	</div>
  		  </table>


<!-- 
  		  <table align=center width=1100>
	 		<center><h1>Data (coming soon)</h1></center>
	 		<center>
		   	<div>
				<img src="./support/MEAD-data.png" height="500" width=0>
		 	</div>
		 	</center>
			<br>
			<p class="text-justify">
				We build the Multi-view Emotional Audio-visual Dataset (MEAD), a talking-face video corpus featuring 60 actors talking with eight different emotions at three different intensity levels (except for neutral). The videos are simultaneously recorded at seven different perspectives in a strictly-controlled environment to provide high-quality details of facial expressions. About 40 hours of audio-visual clips are recorded for each person and view.
			</p>
		   	<div>
				<center><a href="" class="btn btn-outline-secondary">[Download]</a></center>
		   	</div>
  		  </table>
		  <br> -->




  		  <hr>



  		  <table align=center width=1100px>
  			  <tr>
  	              <td>
  					<left>
	  		  <center><h1>Acknowledgements</h1></center>
	  		  This work is supported by the SenseTime-NTU Collaboration Project, Singapore MOE AcRF Tier 1 (2018-T1-002-056), NTU SUG, and NTU NAP.</a>
			</left>
		</td>
		</tr>
		</table>
		<br><br>
</body>
</html>
