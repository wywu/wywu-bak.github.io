
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Look at Boundary: A Boundary-Aware Face Alignment Algorithm</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Wider Facial Landmarks in-the-wild (WFLW) contains 10000 faces (7500 for training and 2500 for testing) with 98 fully manual annotated landmarks. Apart from landmark annotation, out new dataset includes rich attribute annotations, i.e., occlusion, pose, make-up, illumination, blur and expression for comprehensive analysis of existing algorithms. Compare to previous dataset, faces in the proposed dataset introduce large variations in expression, pose and occlusion. We can simply evaluate the robustness of pose, occlusion, and expression on proposed dataset instead of switching between multiple evaluation protocols in different datasets.">
<meta name="keywords" content="LAB; WFLW; wider facial landmarks in-the-wild; facial landmark detection; face alignment; face attributes; deep learning; Convolutional network; computer vision;">
<link rel="author" href="http://wywu.github.io">

<!-- Fonts and stuff -->
<link href="./support/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./support/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./support/iconize.css">
<script async="" src="./support/prettify.js"></script>


</head>


<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1><font size="5">Look at Boundary: A Boundary-Aware Face Alignment Algorithm</font></h1>

	<div class="authors">
	  <a href="http://wywu.github.io">Wayne Wu</a><sup>1,2</sup>&nbsp;&nbsp;
	  <a href="">Chen Qian</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="http://shuoyang1213.me">Shuo Yang</a><sup>3</sup>&nbsp;&nbsp;
	  <a href="">Quan Wang</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="">Yici Cai</a><sup>2</sup>&nbsp;&nbsp;
	  <a href="">Qiang Zhou</a><sup>2</sup>
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="http://www.sist.tsinghua.edu.cn/docinfo_eng/index.jsp">Tsinghua National Laboratory for Information Science and Technology (TNList),<br>Department of Computer Science and Technology, Tsinghua University<br></a>
	  <sup>2</sup><a href="https://www.sensetime.com/?lang=en-us">SenseTime Research<br></a>
	  <sup>3</sup><a href="https://aws.amazon.com/cn/rekognition/">Amazon Rekognition</a>
	</div>
	<ul id="tabs">
		<li><a href="./LAB.html" name="#tab1">LAB</a></li>
		<li><a href="./WFLW.html" name="#tab2">WFLW</a></li>  
	</ul>
	</div>


<!-- 	<div class="venue">International Conference on Computer Vision (<a href="http://pamitc.org/iccv15/" target="_blank">ICCV</a>) 2015, Santiago, Chile</div> -->

      
	<center><img src="./support/WFLF_index.png" border="0" width="92.5%"></center>
	
	<div class="section abstract">
	<h2>Abstract</h2>
	<p>
	Wider Facial Landmarks in-the-wild (WFLW) contains 10000 faces (7500 for training and 2500 for testing) with 98 fully manual annotated landmarks. Apart from landmark annotation, out new dataset includes rich attribute annotations, i.e., occlusion, pose, make-up, illumination, blur and expression for comprehensive analysis of existing algorithms. Compare to previous dataset, faces in the proposed dataset introduce large variations in expression, pose and occlusion. We can simply evaluate the robustness of pose, occlusion, and expression on proposed dataset instead of switching between multiple evaluation protocols in different datasets.
	</p>
	</div>

	<div class="section Download">
	<h2 id="Download">Download</h2>  	
	<p>
	</p><ul>
	<li>WFLW Training and Testing Images <a href="https://drive.google.com/open?id=1hzBd48JIdWTJSsATBEB_eFVvPL1bx6UC">[Google Drive]</a>  <a href="https://pan.baidu.com/s/1paoOpusuyafHY154lqXYrA">[Baidu Drive]</a></li>
	<li><a href="./support/WFLW_annotations.tar.gz">WFLW Face Annotations</a></li>
	</ul>	
	<p></p>
	</div>

	<div class="section Landmark Definition">
	<h2 id="Landmark Definition">Landmark Definition</h2>
	<p>
	<center><img src="./support/WFLW_annotation.png" border="0" width="85%"></center>
	</p>
	</div>


	<div class="section Multi-View Illustration">
	<h2 id="Multi-View Illustration">Multi-View Illustration</h2>
	<p>
	<center><img src="./support/WFLW_multiview.png" border="0" width="80%"></center>
	</p>
	</div>

	<div class="section Results">
	<h2 id="Results">Results</h2>
	<p>
	<center><img src="./support/WFLW_results.png" border="0" width="80%"></center>
	</p>
	</div>


<br>
 <div class="section list">
	<h2>Citation</h2>
	
	<div class="section bibtex">
	  <pre>@inproceedings{wayne2018look,
 author = {Wu, Wayne and Qian, Chen and Yang, Shuo and Wang, Quan and Cai, Yici and Zhou, Qiang},
 title = {Look at Boundary: A Boundary-Aware Face Alignment Algorithm},
 booktitle = {CVPR},
 month = June,
 year = {2018}
} 
	</pre>
	  </div>
      </div>

     <div class="section contact">
	<h2>Contact</h2>
	Wayne Wu<br><a href="mailto:wuwenyan0503@gmail.com">wuwenyan0503@gmail.com</a>
      </div>
    </div>
  </div>

</body></html>
