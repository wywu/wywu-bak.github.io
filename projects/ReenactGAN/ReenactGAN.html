
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>ReenactGAN: Learning to Reenact Faces via Boundary Transfer</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary person’s monocular video input to a target person’s video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source face’s boundary to the target’s boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU).">
<meta name="keywords" content="ReenactGAN; deep fake; Deep Video Portraits; face reenactment; deep learning; Convolutional network; computer vision;">
<link rel="author" href="http://wywu.github.io">

<!-- Fonts and stuff -->
<link href="./support/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./support/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./support/iconize.css">
<script async="" src="./support/prettify.js"></script>


</head>


<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1><font size="5">ReenactGAN: Learning to Reenact Faces via Boundary Transfer</font></h1>

	<div class="authors">
	  <a href="http://wywu.github.io">Wayne Wu</a><sup>*1</sup>&nbsp;&nbsp;
	  <a href="">Yunxuan Zhang</a><sup>*1</sup>&nbsp;&nbsp;
	  <a href="">Cheng Li</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="">Chen Qian</a><sup>1</sup>&nbsp;&nbsp;
	  <a href="">Chen Change Loy</a><sup>2</sup>&nbsp;&nbsp;
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="https://www.sensetime.com/?lang=en-us">SenseTime Research<br></a>
	  <sup>2</sup><a href="">Chinese University of Hong Kong<br></a>
	</div>
	<ul id="tabs">
		<li><a href="./ReenactGAN.html" name="#tab1">ReenactGAN</a></li>
		<!-- <li><a href="./WFLW.html" name="#tab2">WFLW</a></li>   -->
	</ul>
	</div>


<!-- 	<div class="venue">International Conference on Computer Vision (<a href="http://pamitc.org/iccv15/" target="_blank">ICCV</a>) 2015, Santiago, Chile</div> -->

      
      <center><img src="./support/index.png" border="0" width="90%"></center>
      <div class="section abstract">
	<h2>Abstract</h2>
	<p>
We present a novel learning-based framework for face reenactment. The proposed method, known as ReenactGAN, is capable of transferring facial movements and expressions from an arbitrary person’s monocular video input to a target person’s video. Instead of performing a direct transfer in the pixel space, which could result in structural artifacts, we first map the source face onto a boundary latent space. A transformer is subsequently used to adapt the source face’s boundary to the target’s boundary. Finally, a target-specific decoder is used to generate the reenacted target face. Thanks to the effective and reliable boundary-based transfer, our method can perform photo-realistic face reenactment. In addition, ReenactGAN is appealing in that the whole reenactment process is purely feed-forward, and thus the reenactment process can run in real-time (30 FPS on one GTX 1080 GPU).
	</p>
      </div>
<div class="section downloads">
	<h2>Demo</h2><center>
		<iframe width="560" height="315" src="https://www.youtube.com/embed/LBAfeKrHMys" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
      </div></center>
      <div class="section downloads">
	<h2>Downloads</h2>
	<center>
	  <ul>
        <li class="grid">
	      <div class="griditem">
		<a href="./support/ReenactGAN.pdf" target="_blank" class="imageLink"><img src="./support/paper.png"></a><br><a href="./support/ReenactGAN.pdf">Paper</a>
		</div>
	      </li>
        <li class="grid">
	      <div class="griditem">
		<a href="./support/ReenactGAN_Supplementary_Material.pdf" target="_blank" class="imageLink"><img src="./support/sup.png"></a><br><a href="./support/ReenactGAN_Supplementary_Material.pdf">Supplementary Material</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/wywu" target="_blank" class="imageLink"><img src="./support/code.png"></a><br><a href="https://github.com/wywu" target="_blank">Code and Dataset (coming soon)</a>
		</div>
	      </li>
	    </ul>
	    </center>
	    </div>
	    

<br>
 <div class="section list">
	<h2>Citation</h2>
	
	<div class="section bibtex">
	  <pre>@inproceedings{wayne2018reenactgan,
 author = {Wu, Wayne and Zhang, Yunxuan and Li, Cheng and Qian, Chen and Loy, Chen Change},
 title = {ReenactGAN: Learning to Reenact Faces via Boundary Transfer},
 booktitle = {ECCV},
 month = September,
 year = {2018}
} 
	</pre>
	  </div>
      </div>

     <div class="section contact">
	<h2>Contact</h2>
	Wayne Wu<br><a href="mailto:wuwenyan0503@gmail.com">wuwenyan0503@gmail.com</a>
      </div>
    </div>
  </div>

</body></html>
